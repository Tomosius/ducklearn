{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"DuckLearn","text":"<p>DuckLearn is a lightweight, fast, SQL-powered machine learning toolkit built on top of DuckDB, with APIs designed to feel familiar to users of pandas, NumPy, and the scikit-learn ecosystem.</p> <p>The project is intentionally developer-friendly:</p> <ul> <li>Fast, reproducible environments via uv</li> <li>One-command quality gates via pre-commit</li> <li>Strict typing via Mypy and Pyright</li> <li>Consistent formatting and linting via Ruff</li> <li>Documentation via MkDocs Material</li> <li>Conventional commits and versioning via Commitizen</li> </ul> <p>Status: pre-alpha \u2014 APIs will change.</p>"},{"location":"#quick-start","title":"Quick start","text":""},{"location":"#install-runtime","title":"Install (runtime)","text":"<pre><code>pip install ducklearn\n</code></pre>"},{"location":"#clone-and-set-up-for-development","title":"Clone and set up for development","text":"<pre><code>git clone https://github.com/Tomosius/ducklearn\ncd ducklearn\n\n# Install all dependency groups (quality + tests + docs + workflow + release)\nmake dev\n</code></pre> <p>Run the full local quality gate (same as pre-commit):</p> <pre><code>make check\n</code></pre> <p>Run tests:</p> <pre><code>make test\n</code></pre> <p>Serve docs:</p> <pre><code>make docs-serve\n</code></pre>"},{"location":"#requirements","title":"Requirements","text":"<ul> <li>Python: 3.12\u20133.14 (matches <code>requires-python = \"&gt;=3.12,&lt;3.15\"</code>)</li> <li><code>uv</code> installed (recommended): https://docs.astral.sh/uv/</li> <li>Git (for pre-commit hooks)</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<p>This repository uses the <code>src/</code> layout:</p> <pre><code>src/\n  ducklearn/\n    __init__.py\ntests/\npyproject.toml\nuv.lock\nMakefile\n.pre-commit-config.yaml\nmkdocs.yml\n</code></pre>"},{"location":"#dependency-groups-pep-735","title":"Dependency groups (PEP 735)","text":"<p>Dependencies are organized into groups in <code>pyproject.toml</code>:</p> <ul> <li><code>quality</code>: linters, type checkers, doc quality tools</li> <li><code>tests</code>: pytest, coverage, hypothesis, etc.</li> <li><code>docs</code>: mkdocs + plugins</li> <li><code>workflow</code>: pre-commit, commitizen</li> <li><code>release</code>: build/wheel tooling for packaging</li> </ul> <p>In development, the recommended setup is installing all groups:</p> <pre><code>uv sync --all-groups\n</code></pre>"},{"location":"#the-makefile","title":"The Makefile","text":"<p>The Makefile is the \u201cfront door\u201d for day-to-day workflows. It is uv-driven and intentionally mirrors what CI would do.</p>"},{"location":"#why-this-matters","title":"Why this matters","text":"<ul> <li>You run the same toolchain locally as CI.</li> <li>Everything is pinned via <code>uv.lock</code>.</li> <li>Pre-commit is the single orchestration layer for checks (consistent results).</li> </ul>"},{"location":"#targets","title":"Targets","text":""},{"location":"#make-install","title":"<code>make install</code>","text":"<p>Installs the runtime environment (default dependency set):</p> <pre><code>make install\n</code></pre> <p>Equivalent to:</p> <pre><code>uv sync\n</code></pre> <p>Use this if you only want to use DuckLearn (not develop it).</p>"},{"location":"#make-dev","title":"<code>make dev</code>","text":"<p>Installs the full development environment (all dependency groups) and installs git hooks for pre-commit:</p> <pre><code>make dev\n</code></pre> <p>Equivalent to:</p> <pre><code>uv sync --all-groups\nuv run pre-commit install\n</code></pre>"},{"location":"#make-check","title":"<code>make check</code>","text":"<p>Runs the full pre-commit suite across the entire repository:</p> <pre><code>make check\n</code></pre> <p>This is the canonical local \u201cquality gate\u201d. It runs:</p> <ul> <li>formatting/linting (Ruff)</li> <li>type checks (Mypy, Pyright)</li> <li>docstring coverage gate (Interrogate)</li> <li>and any other hooks configured in <code>.pre-commit-config.yaml</code></li> </ul> <p>If pre-commit modifies files (e.g., formatting), re-run <code>make check</code> until clean.</p>"},{"location":"#make-check-fast","title":"<code>make check-fast</code>","text":"<p>Runs pre-commit only on currently staged files:</p> <pre><code>make check-fast\n</code></pre> <p>This is the fastest loop when you are iterating on a small change.</p>"},{"location":"#make-fix","title":"<code>make fix</code>","text":"<p>Runs pre-commit across the repo, but does not fail the Make target (useful for applying autofixes quickly):</p> <pre><code>make fix\n</code></pre> <p>This is a convenience \u201capply fixes then re-check\u201d helper.</p>"},{"location":"#make-test","title":"<code>make test</code>","text":"<p>Runs the test suite:</p> <pre><code>make test\n</code></pre> <p>Equivalent to:</p> <pre><code>uv run pytest\n</code></pre>"},{"location":"#make-test-cov","title":"<code>make test-cov</code>","text":"<p>Runs tests with coverage output:</p> <pre><code>make test-cov\n</code></pre>"},{"location":"#make-docs-serve","title":"<code>make docs-serve</code>","text":"<p>Serves documentation locally with live reload:</p> <pre><code>make docs-serve\n</code></pre>"},{"location":"#make-docs-build","title":"<code>make docs-build</code>","text":"<p>Builds the documentation site:</p> <pre><code>make docs-build\n</code></pre>"},{"location":"#make-docs-deploy","title":"<code>make docs-deploy</code>","text":"<p>Deploys docs to GitHub Pages (requires appropriate repo permissions):</p> <pre><code>make docs-deploy\n</code></pre>"},{"location":"#make-prerelease","title":"<code>make prerelease</code>","text":"<p>Runs a multi-Python matrix for pre-release confidence. This creates separate environments per Python minor and runs checks + tests in each:</p> <pre><code>make prerelease\n</code></pre> <p>Internally it uses:</p> <ul> <li><code>.venv-py3.12</code></li> <li><code>.venv-py3.13</code></li> <li><code>.venv-py3.14</code></li> </ul> <p>and runs:</p> <ul> <li><code>pre-commit run --all-files</code></li> <li><code>pytest</code></li> </ul> <p>This is especially useful before tagging / publishing.</p> <p>Note: this does not replace CI, but it makes local validation very strong.</p>"},{"location":"#pre-commit-what-runs-in-make-check","title":"Pre-commit (what runs in <code>make check</code>)","text":"<p>Pre-commit is configured in <code>.pre-commit-config.yaml</code>.</p> <p>You can always run it directly:</p> <pre><code>uv run pre-commit run --all-files --show-diff-on-failure\n</code></pre> <p>Install hooks (done by <code>make dev</code>):</p> <pre><code>uv run pre-commit install\n</code></pre>"},{"location":"#typing-and-style","title":"Typing and style","text":"<p>DuckLearn aims for:</p> <ul> <li>Ruff formatting + lint rules</li> <li>Mypy strict typing</li> <li>Pyright strict mode</li> <li>Docstring coverage gate via Interrogate</li> </ul> <p>If you see a failure, prefer fixing the underlying issue rather than weakening the checks. If a rule is too strict for a valid reason, document the exception and scope it narrowly (per-file ignore, or targeted config).</p>"},{"location":"#versioning-and-changelog-commitizen","title":"Versioning and changelog (Commitizen)","text":"<p>Commitizen helps enforce conventional commits and can manage version bumps.</p> <p>Typical flows:</p> <p>Create an interactive conventional commit:</p> <pre><code>uv run cz commit\n</code></pre> <p>Bump version (updates project version + changelog if configured):</p> <pre><code>uv run cz bump\n</code></pre> <p>Your exact Commitizen behavior is configured under <code>[tool.commitizen]</code> in <code>pyproject.toml</code>.</p>"},{"location":"#packaging-and-release","title":"Packaging and release","text":"<p>To build distribution artifacts (wheel + sdist):</p> <pre><code>uv run python -m build\n</code></pre>"},{"location":"#do-you-need-twine","title":"Do you need Twine?","text":"<p>Twine is typically used to upload built artifacts to PyPI:</p> <pre><code>uv run twine upload dist/*\n</code></pre> <p>If you publish via GitHub Actions / trusted publishing, you may not need Twine locally. Keep it in <code>release</code> only if you actually upload from your machine.</p>"},{"location":"#git-staging-tips","title":"Git staging tips","text":"<p>To stage only modified/deleted tracked files (not new files), use:</p> <pre><code>git add -u\n</code></pre> <p>To stage specific files:</p> <pre><code>git add path/to/file1 path/to/file2\n</code></pre> <p>Avoid <code>git add -A</code> if you don\u2019t want to add newly created files.</p>"},{"location":"#license","title":"License","text":"<p>DuckLearn is licensed under the Apache License 2.0.</p> <p>See the <code>LICENSE</code> file for full text.</p>"},{"location":"#support-roadmap","title":"Support / roadmap","text":"<p>If you spot gaps, rough edges, or want to propose a feature:</p> <ul> <li>open an issue on GitHub</li> <li>include a minimal reproducible example when reporting bugs</li> <li>include target audience and motivation for feature requests</li> </ul>"},{"location":"architecture/code_style/","title":"Code Style","text":""},{"location":"architecture/code_style/#ducklearn-code-style-guide","title":"DuckLearn Code Style Guide","text":""},{"location":"architecture/code_style/#introduction","title":"Introduction","text":"<p>DuckLearn follows strict, professional, and explicit coding rules to ensure maintainability, readability, and reliability across the entire project. This guide applies to Python, SQL, SQLGlot, Markdown, and YAML. It fully aligns with the tooling configured in <code>pyproject.toml</code>: ruff, mypy, pylint, pyright, vulture, and pytest.</p>"},{"location":"architecture/code_style/#1-python-coding-standards","title":"1. Python Coding Standards","text":""},{"location":"architecture/code_style/#11-line-length","title":"1.1 Line Length","text":"<ul> <li>Maximum: 79 characters</li> <li>Applies to code, docstrings, comments, examples</li> </ul>"},{"location":"architecture/code_style/#12-naming-conventions","title":"1.2 Naming Conventions","text":"<ul> <li>Use full, descriptive names everywhere.</li> <li>Only scikit-learn compatibility methods may use short names:<ul> <li><code>fit</code>, <code>transform</code>, <code>fit_transform</code></li> <li><code>get_params</code>, <code>set_params</code></li> </ul> </li> </ul> <p>Examples:</p> Avoid Prefer df input_dataframe vals transformed_values sql_expr sql_transformation_expression"},{"location":"architecture/code_style/#13-docstrings","title":"1.3 Docstrings","text":"<ul> <li>Required for all public modules, classes, methods, functions.</li> <li>Google or NumPy style acceptable.</li> <li>Must document: Purpose, Args, Returns, Raises, Examples.</li> </ul>"},{"location":"architecture/code_style/#14-inline-comments","title":"1.4 Inline Comments","text":"<ul> <li>Required for non-obvious logic.</li> <li>Must be complete sentences.</li> <li>Avoid explaining the obvious.</li> </ul>"},{"location":"architecture/code_style/#15-imports","title":"1.5 Imports","text":"<ul> <li>Order:<ol> <li>Standard library</li> <li>Third\u2011party</li> <li>Local modules</li> </ol> </li> <li>Alphabetical within each group.</li> </ul>"},{"location":"architecture/code_style/#16-typing-rules","title":"1.6 Typing Rules","text":"<ul> <li>Full type annotations required.</li> <li>Mypy strict mode enforced:<ul> <li>No untyped definitions</li> <li>No incomplete types</li> <li>No unnecessary ignores</li> <li>Warn on unreachable code</li> </ul> </li> </ul>"},{"location":"architecture/code_style/#2-sql-coding-standards","title":"2. SQL Coding Standards","text":""},{"location":"architecture/code_style/#21-line-length","title":"2.1 Line Length","text":"<ul> <li>Soft max: 90 characters</li> <li>Prefer readable multi-line formatting</li> </ul>"},{"location":"architecture/code_style/#22-naming","title":"2.2 Naming","text":"<ul> <li>Raw SQL: UPPERCASE keywords</li> <li>SQLGlot: lowercase keywords (tooling requirement)</li> <li>Identifiers: snake_case</li> </ul>"},{"location":"architecture/code_style/#23-query-style","title":"2.3 Query Style","text":"<ul> <li>Use explicit JOIN</li> <li>Always alias tables</li> <li>Avoid SELECT *</li> </ul>"},{"location":"architecture/code_style/#3-sqlglot-conventions","title":"3. SQLGlot Conventions","text":""},{"location":"architecture/code_style/#31-ast-construction","title":"3.1 AST Construction","text":"<ul> <li>Prefer programmatic AST building</li> <li>Avoid raw SQL strings unless necessary</li> </ul>"},{"location":"architecture/code_style/#32-naming","title":"3.2 Naming","text":"<ul> <li>Use fully descriptive variable names</li> </ul>"},{"location":"architecture/code_style/#4-markdown-conventions","title":"4. Markdown Conventions","text":""},{"location":"architecture/code_style/#41-structure","title":"4.1 Structure","text":"<ul> <li>Only one H1 per file</li> <li>Proper hierarchy: H1 \u2192 H2 \u2192 H3</li> </ul>"},{"location":"architecture/code_style/#42-line-length","title":"4.2 Line Length","text":"<ul> <li>Aim for \u2264 80 characters (text only)</li> </ul>"},{"location":"architecture/code_style/#43-lists","title":"4.3 Lists","text":"<ul> <li>Unordered lists for unordered concepts</li> <li>Ordered lists when order matters</li> </ul>"},{"location":"architecture/code_style/#5-yaml-conventions","title":"5. YAML Conventions","text":""},{"location":"architecture/code_style/#51-formatting","title":"5.1 Formatting","text":"<ul> <li>2-space indentation</li> <li>Snake_case keys</li> <li>Maximum line length: 79 characters</li> </ul>"},{"location":"architecture/code_style/#52-comments","title":"5.2 Comments","text":"<ul> <li>Clarify configuration intent</li> <li>Avoid long inline comments</li> </ul>"},{"location":"architecture/code_style/#6-general-engineering-principles","title":"6. General Engineering Principles","text":""},{"location":"architecture/code_style/#61-explicit-implicit","title":"6.1 Explicit &gt; Implicit","text":"<p>No magic behavior.</p>"},{"location":"architecture/code_style/#62-one-responsibility-per-function","title":"6.2 One Responsibility per Function","text":"<p>Functions should stay small and clear.</p>"},{"location":"architecture/code_style/#63-dry-but-not-too-dry","title":"6.3 DRY (but not too DRY)","text":"<p>Avoid unnecessary abstraction.</p>"},{"location":"architecture/code_style/#64-fail-loudly","title":"6.4 Fail Loudly","text":"<p>Raise informative exceptions early.</p>"},{"location":"architecture/code_style/#65-avoid-side-effects","title":"6.5 Avoid Side Effects","text":"<p>Ensure predictable, deterministic behavior.</p>"},{"location":"architecture/code_style/#7-compatibility-rules","title":"7. Compatibility Rules","text":""},{"location":"architecture/code_style/#71-scikit-learn-api","title":"7.1 Scikit-Learn API","text":"<p>The following methods must always exist on DuckLearn transformers:</p> <ul> <li>fit</li> <li>transform</li> <li>fit_transform</li> <li>get_params</li> <li>set_params</li> </ul>"},{"location":"architecture/code_style/#72-linterformatter-compatibility","title":"7.2 Linter/Formatter Compatibility","text":"<p>All code must pass:</p> <ul> <li>ruff (lint + format)</li> <li>pylint</li> <li>mypy</li> <li>pyright</li> <li>docformatter</li> <li>vulture</li> <li>pytest</li> </ul>"},{"location":"architecture/code_style/#8-summary","title":"8. Summary","text":"<p>This guide ensures:</p> <ul> <li>Clean, consistent, maintainable code</li> <li>Fully typed, fully linted, fully documented implementations</li> <li>SQL and SQLGlot consistency</li> <li>Predictable API behavior</li> <li>Easy onboarding for contributors</li> </ul> <p>Always follow this guide before committing or reviewing code.</p>"},{"location":"architecture/pipeline/","title":"DuckLearn Pipeline Design","text":""},{"location":"architecture/pipeline/#overview","title":"Overview","text":"<p>DuckLearn pipelines define a deterministic, SQL-native sequence of transformations that operate entirely within a relational engine such as DuckDB. Unlike scikit-learn pipelines, DuckLearn pipelines do not operate on NumPy arrays but instead construct a CTE-based SQL graph.</p> <p>A pipeline is:</p> <ul> <li>Lazy (builds SQL, does not execute)</li> <li>Immutable (each step produces a new SQL AST)</li> <li>Fully traceable (debuggable SQL)</li> <li>Engine-agnostic (SQLGlot translation layer)</li> </ul>"},{"location":"architecture/pipeline/#1-pipeline-philosophy","title":"1. Pipeline Philosophy","text":"<p>DuckLearn pipelines follow three principles:</p>"},{"location":"architecture/pipeline/#1-predictability","title":"1. Predictability","text":"<p>Every pipeline step must produce a reproducible SQL transformation.</p>"},{"location":"architecture/pipeline/#2-transparency","title":"2. Transparency","text":"<p>Users can inspect or print SQL at any stage.</p>"},{"location":"architecture/pipeline/#3-compatibility","title":"3. Compatibility","text":"<p>Pipelines respect scikit-learn conventions but extend them into SQL.</p>"},{"location":"architecture/pipeline/#2-pipeline-structure","title":"2. Pipeline Structure","text":"<p>A pipeline is composed of ordered named steps, for example:</p> <pre><code>Pipeline(\n    [\n        (\"selector\", ColumnSelector(columns=[\"a\", \"b\"])),\n        (\"scale\", StandardScaler()),\n        (\"encode\", OneHotEncoder())\n        ]\n    )\n</code></pre> <p>Each step implements:</p> <ul> <li><code>fit()</code></li> <li><code>transform()</code></li> </ul> <p>And returns SQL AST nodes.</p>"},{"location":"architecture/pipeline/#3-execution-model","title":"3. Execution Model","text":"<p>Internally:</p> <ol> <li>Each step receives a SQLGlot expression representing the current table.</li> <li>It produces a new SQLGlot expression.</li> <li>The pipeline assembles these expressions into a CTE graph.</li> <li>Execution occurs only at:<ul> <li><code>execute()</code></li> <li><code>to_dataframe()</code></li> <li><code>to_arrow()</code></li> </ul> </li> </ol>"},{"location":"architecture/pipeline/#31-example-cte-graph","title":"3.1 Example CTE Graph","text":"<pre><code>WITH\n    step_00 AS (SELECT * FROM raw_input),\n    step_01 AS (SELECT col1, col2_normalized AS col2 FROM step_00),\n    step_02 AS (... FROM step_01),\n    step_03 AS (... FROM step_02)\nSELECT * FROM step_03;\n</code></pre> <p>Each pipeline step maps to exactly one CTE.</p>"},{"location":"architecture/pipeline/#4-fit-transform-behavior","title":"4. Fit / Transform Behavior","text":""},{"location":"architecture/pipeline/#fit","title":"<code>fit()</code>","text":"<ul> <li>Computes metadata (e.g., min/max, categories)</li> <li>Does not modify SQL</li> </ul>"},{"location":"architecture/pipeline/#transform","title":"<code>transform()</code>","text":"<ul> <li>Emits SQLGlot expressions using learned metadata</li> </ul>"},{"location":"architecture/pipeline/#fit_transform","title":"<code>fit_transform()</code>","text":"<ul> <li>Equivalent to scikit-learn semantics</li> </ul>"},{"location":"architecture/pipeline/#5-partial-pipelines","title":"5. Partial Pipelines","text":"<p>DuckLearn supports:</p> <ul> <li>Returning SQL for an intermediate step</li> <li>Inspecting AST of any step</li> <li>Debugging individual transformers</li> </ul> <p>Example:</p> <pre><code>pipeline.get_step_sql(\"scale\")\n</code></pre>"},{"location":"architecture/pipeline/#6-validation-rules","title":"6. Validation Rules","text":"<p>Pipeline construction enforces:</p> <ul> <li>Unique step names</li> <li>Each step must be a TransformerMixin subclass</li> <li>Order preserved</li> <li>Learned attributes must end in <code>_</code></li> <li>Transformers must not mutate the incoming expression</li> </ul>"},{"location":"architecture/pipeline/#7-error-handling","title":"7. Error Handling","text":"<p>The pipeline raises explicit errors:</p> Error Type Condition MissingColumnError transform references nonexistent columns InvalidStepError step is not a transformer FitRequiredError transform before fit EngineExecutionError SQL engine rejects generated SQL"},{"location":"architecture/pipeline/#8-future-enhancements","title":"8. Future Enhancements","text":"<ul> <li>Branching pipelines (DAG, not just linear)</li> <li>Hyperparameter search integration</li> <li>Optimized SQL fusion (combine compatible CTEs)</li> <li>Visual pipeline graph export</li> </ul>"},{"location":"architecture/pipeline/#summary","title":"Summary","text":"<p>DuckLearn pipelines:</p> <ul> <li>Build SQL lazily</li> <li>Represent transformations as a CTE graph</li> <li>Maintain strict sklearn-compatible behavior</li> <li>Provide full introspection and debuggability</li> <li>Serve as the backbone of the DuckLearn execution model</li> </ul> <p>This file defines how pipelines should be implemented and extended across the DuckLearn project.</p>"}]}